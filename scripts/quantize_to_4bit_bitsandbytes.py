#!/usr/bin/env python3
"""
Quantize INT8 PyTorch Models to 4-bit using BitsAndBytes
Supports NF4 (normalized float 4-bit) and FP4 (float 4-bit) quantization
for ultra-lightweight deployment on edge devices.

Reference: https://huggingface.co/docs/bitsandbytes/en/reference/nn/linear4bit
"""

import argparse
import json
import sys
import warnings
from pathlib import Path
from typing import Literal, Tuple

import torch
import torch.nn as nn

# Add parent directory to path to import src/lib
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.lib import (
    calculate_model_size,
    load_model_checkpoint,
    setup_logger,
    verify_and_setup_gpu,
)

logger = setup_logger(__name__)

# Suppress PyTorch's TypedStorage deprecation warning (internal, not in user code)
warnings.filterwarnings("ignore", category=UserWarning, message=".*TypedStorage.*")


def quantize_model_4bit_nf4(
    model: nn.Module,
    device: str = "cuda",
    double_quant: bool = True,
    quant_type: Literal["nf4", "fp4"] = "nf4",
) -> nn.Module:
    """
    Quantize model to 4-bit using BitsAndBytes Linear4bit layers.

    Supports two 4-bit quantization schemes:
    - NF4 (Normalized Float 4-bit): Better for normally distributed weights
    - FP4 (Float 4-bit): Standard 4-bit float with sign bit

    Important: BitsAndBytes quantizes weights in-place during computation,
    so saved model file size remains similar. The benefit is in inference speed
    and memory usage during inference (weights loaded as 4-bit, not 32-bit).

    Args:
        model: PyTorch model to quantize
        device: Device to use (always cuda)
        double_quant: Whether to use double quantization (quantize the scale factors)
        quant_type: Quantization type - "nf4" (recommended) or "fp4"

    Returns:
        Quantized model with Linear4bit layers
    """

    logger.info("ðŸ”„ Quantizing model to 4-bit %s...", quant_type.upper())

    try:
        from bitsandbytes.nn import Linear4bit
    except ImportError:
        logger.error("âœ— BitsAndBytes not installed. Install with: pip install bitsandbytes")
        return None

    model = model.to(device)
    model.eval()

    # Replace Linear layers with Linear4bit
    quantized_count = 0
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            # Create 4-bit quantized linear layer
            # BitsAndBytes Linear4bit wraps the original layer
            q_linear = Linear4bit(
                module.in_features,
                module.out_features,
                bias=module.bias is not None,
                compute_dtype=torch.float16,  # Internal computation in float16
                compress_statistics=double_quant,
                quant_type=quant_type,
            )

            # Copy weights and bias
            if module.weight is not None:
                q_linear.weight = module.weight
            if module.bias is not None:
                q_linear.bias = module.bias

            # Replace in model
            parent_name = ".".join(name.split(".")[:-1])
            child_name = name.split(".")[-1]

            parent = model
            for attr in parent_name.split("."):
                if attr:
                    parent = getattr(parent, attr)

            if parent_name:
                setattr(parent, child_name, q_linear)
            else:
                setattr(model, child_name, q_linear)

            quantized_count += 1
            if quantized_count <= 5:  # Print first 5
                logger.info(
                    "  â†’ Quantized layer: %s (%d â†’ %d)",
                    name,
                    module.in_features,
                    module.out_features,
                )
            elif quantized_count == 6:
                logger.info("  â†’ ... and %d more layers", quantized_count - 5)

    logger.info("âœ“ Replaced %d layers with 4-bit quantization", quantized_count)
    return model


def quantize_model_4bit_gptq(
    model: nn.Module,
    device: str = "cuda",
    bits: int = 4,
    group_size: int = 128,
) -> nn.Module:
    """
    Quantize model to 4-bit using GPTQ (Generative Pretrained Transformer Quantization).

    GPTQ provides better accuracy preservation than simple 4-bit quantization
    by optimizing scale factors based on Hessian information.

    Args:
        model: PyTorch model to quantize
        device: Device to use (always cuda)
        bits: Bit width (default 4)
        group_size: Group size for quantization (default 128)

    Returns:
        Quantized model
    """

    try:
        from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig  # noqa: F401
    except ImportError:
        return None

    # GPTQ quantization requires specific dataset calibration
    # This is a simplified version - full GPTQ requires training data

    # Note: Full GPTQ implementation would require dataset loading
    # For now, return original model with warning
    return model


def load_model_for_quantization(
    model_path: str,
    model_type: str,
    device: str = "cuda",
) -> Tuple[nn.Module, int]:
    """
    Load a trained model for quantization.

    Args:
        model_path: Path to model checkpoint
        model_type: Type of model (cnn, rnn, hiercode, etc.)
        device: Device to use

    Returns:
        Tuple of (model, num_classes)
    """
    model_path = Path(model_path)
    if not model_path.exists():
        return None, None

    # Use library function to load model checkpoint
    try:
        model, num_classes, _info = load_model_checkpoint(str(model_path), model_type)
        model = model.to(device)
        model.eval()
        return model, num_classes
    except Exception:
        return None, None


def save_4bit_model(
    model: nn.Module,
    output_path: str,
    original_size: float,
    quant_method: str,
    quant_type: str,
    model_info: dict,
) -> Tuple[str, dict]:
    """
    Save 4-bit quantized model with metadata.

    Args:
        model: Quantized model
        output_path: Path to save model
        original_size: Original model size in bytes
        quant_method: Quantization method (nf4, fp4, gptq)
        quant_type: Specific quantization type
        model_info: Additional model information

    Returns:
        Tuple of (saved_path, metadata)
    """
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Save model state dict
    torch.save(model.state_dict(), output_path)

    # Measure file size
    quantized_size = output_path.stat().st_size
    size_reduction = (1 - quantized_size / original_size) * 100

    # Save metadata
    metadata = {
        "model_type": model_info.get("model_type", "unknown"),
        "num_classes": model_info.get("num_classes", 3036),
        "quantization_method": quant_method,
        "quantization_type": quant_type,
        "original_size_mb": original_size / 1e6,
        "quantized_size_mb": quantized_size / 1e6,
        "size_reduction_percent": size_reduction,
        "from_model": model_info.get("from_model", ""),
        "compute_dtype": "float16",
        "deployment_note": f"4-bit {quant_type.upper()} quantized model - ultra-lightweight for edge devices",
    }

    metadata_path = output_path.with_suffix(".json")
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=2)

    return str(output_path), metadata


def main():
    parser = argparse.ArgumentParser(
        description="Quantize INT8/FP32 Models to 4-bit using BitsAndBytes",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Quantize to 4-bit NF4 (recommended)
  python quantize_to_4bit_bitsandbytes.py --model-path training/cnn/best_cnn.pth --model-type cnn --method nf4

  # Quantize to 4-bit FP4
  python quantize_to_4bit_bitsandbytes.py --model-path training/cnn/best_cnn.pth --model-type cnn --method fp4

  # Quantize with double quantization (compress scale factors)
  python quantize_to_4bit_bitsandbytes.py --model-path training/cnn/best_cnn.pth --model-type cnn --double-quant

  # Save to custom path
  python quantize_to_4bit_bitsandbytes.py --model-path training/cnn/best_cnn.pth --model-type cnn \
    --output training/cnn/cnn_4bit_nf4.pth

  # Quantize HierCode model
  python quantize_to_4bit_bitsandbytes.py --model-path training/hiercode/best_hiercode.pth --model-type hiercode
        """,
    )

    parser.add_argument(
        "--model-path",
        required=True,
        type=str,
        help="Path to model checkpoint (INT8 or float32)",
    )
    parser.add_argument(
        "--model-type",
        required=True,
        type=str,
        choices=["cnn", "rnn", "hiercode", "hiercode-higita", "qat", "radical-rnn", "vit"],
        help="Model architecture type",
    )
    parser.add_argument(
        "--method",
        type=str,
        choices=["nf4", "fp4", "gptq"],
        default="nf4",
        help="4-bit quantization method (default: nf4 - recommended)",
    )
    parser.add_argument(
        "--double-quant",
        action="store_true",
        help="Use double quantization (quantize scale factors)",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="Output path (default: auto-generated with verbose naming)",
    )

    args = parser.parse_args()

    # Verify GPU
    device = verify_and_setup_gpu()

    # Load model

    model, num_classes = load_model_for_quantization(
        args.model_path,
        args.model_type,
        device=device,
    )

    if model is None:
        return

    # Calculate original size
    original_size = calculate_model_size(model)

    # Quantize model
    if args.method in ["nf4", "fp4"]:
        quantized_model = quantize_model_4bit_nf4(
            model,
            device=device,
            double_quant=args.double_quant,
            quant_type=args.method,
        )
    elif args.method == "gptq":
        quantized_model = quantize_model_4bit_gptq(model, device=device)
    else:
        return

    if quantized_model is None:
        return

    # Generate output path if not specified
    if args.output is None:
        method_name = args.method.upper()
        double_quant_suffix = "_dq" if args.double_quant else ""
        output_filename = f"{args.model_type}_int8_4bit_{method_name}{double_quant_suffix}.pth"
        output_path = Path(args.model_path).parent / output_filename
    else:
        output_path = Path(args.output)

    # Save quantized model
    saved_path, metadata = save_4bit_model(
        quantized_model,
        str(output_path),
        original_size,
        quant_method=args.method,
        quant_type=args.method.upper(),
        model_info={
            "model_type": args.model_type,
            "num_classes": num_classes,
            "from_model": str(args.model_path),
        },
    )

    # Print summary

    if args.double_quant:
        pass


if __name__ == "__main__":
    main()
