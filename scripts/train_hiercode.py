#!/usr/bin/env python3
"""
HierCode (Hierarchical Codebook) Training for Kanji Recognition
Novel approach for zero-shot and efficient character recognition.
Target: <2 MB model, â‰¥97% accuracy, zero-shot capability

Features:
- Automatic checkpoint management with resume from latest checkpoint
- Dataset auto-detection (combined_all_etl, etl9g, etl8g, etl7, etl6, etl1)
- NVIDIA GPU required with CUDA optimizations enabled

Configuration parameters are documented inline.
Paper: "HierCode: A Lightweight Hierarchical Codebook for Zero-shot
        Chinese Text Recognition" (arXiv:2403.13761, March 2024)
For more info: See HIERCODE_DISCOVERY.md and GITHUB_IMPLEMENTATION_REFERENCES.md
"""

import argparse
import sys
from pathlib import Path
from typing import Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F  # noqa: N812
from torch.utils.data import DataLoader
from tqdm import tqdm

# Add parent directory to path to import src/lib
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.lib import (
    CheckpointManager,
    HierCodeConfig,
    create_data_loaders,
    get_dataset_directory,
    get_optimizer,
    get_scheduler,
    load_best_model_for_testing,
    load_chunked_dataset,
    save_best_model,
    save_config,
    save_training_results,
    setup_checkpoint_arguments,
    setup_logger,
    verify_and_setup_gpu,
)

logger = setup_logger(__name__)

# ============================================================================
# HIERCODE COMPONENTS
# ============================================================================


class HierarchicalCodebook(nn.Module):
    """
    Hierarchical Codebook for efficient character encoding.

    ==================== HIERARCHICAL CODEBOOK THEORY ====================

    Problem: 3000+ character classes â†’ need efficient encoding
    Solution: Organize codebook hierarchically using binary tree structure

    Example (for 1024 codewords, depth=10):
    - Root node represents start of tree
    - Each non-leaf node has 2 children (binary tree branching)
    - Leaf nodes (2^10 = 1024) represent unique codewords
    - Path from root to leaf: 10-bit binary code to reach any codeword

    Benefits:
    1. Memory efficient: 1024 codewords â†’ 10 bits vs 10 bits at top
    2. Hierarchical: Related characters close in tree (similar codes)
    3. Multi-hot: Use k paths (k active codewords) per character
    4. Learnable: Codebook learned during training

    ==================== MULTI-HOT ENCODING ====================

    Instead of one-hot (exactly 1 codeword), use multi-hot (k codewords):
    - Each character represented by k paths through hierarchy
    - Example: æ˜Ž (bright) = [codeword_3, codeword_7, codeword_15, codeword_22, codeword_31]
    - Captures character ambiguity and compositional structure
    """

    def __init__(self, total_size: int, codebook_dim: int, depth: int, temperature: float = 0.1):
        super().__init__()

        self.total_size = total_size
        self.codebook_dim = codebook_dim
        self.depth = depth  # Depth of binary tree
        self.temperature = temperature

        # Number of nodes at each level in binary tree
        # Level 0 (root): 1 node
        # Level 1: 2 nodes
        # Level d: 2^d nodes (leaves)
        self.num_leaves = 2**depth

        # Ensure total_size matches number of leaves
        if total_size > self.num_leaves:
            raise ValueError(f"total_size ({total_size}) > num_leaves ({self.num_leaves})")

        # Initialize codebook: learnable embeddings for each leaf node
        # Shape: (num_leaves, codebook_dim)
        self.codebook = nn.Parameter(
            torch.randn(self.num_leaves, codebook_dim) / (codebook_dim**0.5)
        )

        # Learnable routing weights for hierarchical paths
        # Used to route to active codewords
        self.register_buffer("routing_weights", torch.ones(depth))

    def get_codeword(self, codeword_id: int) -> torch.Tensor:
        """Get a single codeword embedding"""
        if codeword_id >= self.total_size:
            codeword_id = codeword_id % self.total_size
        return self.codebook[codeword_id]

    def get_multi_hot_codes(
        self, batch_size: int, k: int, device: torch.device
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Sample k active codewords for each sample in batch.

        Returns:
        - codes: (batch_size, k, codebook_dim) - active codeword embeddings
        - code_ids: (batch_size, k) - indices of active codewords
        """
        # Randomly select k codewords for each sample
        code_ids = torch.randint(0, self.total_size, (batch_size, k), device=device)
        codes = self.codebook[code_ids]

        return codes, code_ids


class HierCodeBackbone(nn.Module):
    """
    Lightweight CNN backbone for HierCode.

    ==================== BACKBONE PURPOSE ====================

    - Extract visual features from 64x64 image
    - Output: Feature vector for codebook selection
    - Efficiency: Minimal parameters to keep model <2 MB
    """

    def __init__(self, backbone_type: str = "lightweight_cnn", output_dim: int = 256):
        super().__init__()

        self.backbone_type = backbone_type
        self.output_dim = output_dim

        if backbone_type == "lightweight_cnn":
            # Lightweight depthwise separable CNN
            self.features = nn.Sequential(
                # 64x64 -> 32x32
                nn.Conv2d(1, 16, 3, stride=2, padding=1, groups=1, bias=False),
                nn.BatchNorm2d(16),
                nn.ReLU(inplace=True),
                # 32x32 -> 16x16
                nn.Conv2d(16, 32, 3, stride=2, padding=1, groups=16, bias=False),
                nn.BatchNorm2d(32),
                nn.ReLU(inplace=True),
                nn.Conv2d(32, 32, 1, bias=False),
                nn.BatchNorm2d(32),
                nn.ReLU(inplace=True),
                # 16x16 -> 8x8
                nn.Conv2d(32, 64, 3, stride=2, padding=1, groups=32, bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 64, 1, bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                # Global pooling
                nn.AdaptiveAvgPool2d(1),
            )

            self.proj = nn.Linear(64, output_dim)

        else:
            raise ValueError(f"Unknown backbone type: {backbone_type}")

    def forward(self, x):
        # Input: (batch, 1, 64, 64)
        x = self.features(x)  # (batch, 64, 1, 1)
        x = x.view(x.size(0), -1)  # (batch, 64)
        x = self.proj(x)  # (batch, output_dim)
        return x


class HierCodeClassifier(nn.Module):
    """
    HierCode Classifier: Hierarchical Codebook + Character Classification.

    ==================== ARCHITECTURE ====================

    1. Backbone (CNN):
       - Extract visual features from image
       - Output: feature vector

    2. Feature Projection:
       - Project features to codebook space
       - Output: Gumbel-softmax for differentiable selection

    3. Codebook Selection:
       - Select k active codewords via multi-hot encoding
       - Use Gumbel-softmax trick for hard selection with gradient

    4. Prototype Learning (Optional):
       - Learn prototypical representations of character classes
       - Match selected codewords to prototypes

    5. Zero-shot Learning (Optional):
       - Use radical decomposition for unseen characters
       - Combine radicals to predict new character

    ==================== TRAINING OBJECTIVES ====================

    1. Classification: Predict character class from selected codewords
    2. Prototype matching: Selected codes should match class prototype
    3. Codebook efficiency: Minimize number of active codes needed
    4. Radical consistency (optional): Maintain radical structure
    """

    def __init__(self, num_classes: int, config: HierCodeConfig):
        super().__init__()

        self.num_classes = num_classes
        self.config = config

        # ==================== BACKBONE ====================
        self.backbone = HierCodeBackbone(
            backbone_type=config.backbone_type, output_dim=config.backbone_output_dim
        )

        # ==================== CODEBOOK ====================
        self.codebook = HierarchicalCodebook(
            total_size=config.codebook_total_size,
            codebook_dim=config.codebook_dim,
            depth=config.hierarch_depth,
            temperature=config.temperature,
        )

        # ==================== CODEWORD SELECTION ====================
        # Project backbone features to codebook selection logits
        # Select k codewords per character
        self.codeword_selector = nn.Sequential(
            nn.Linear(config.backbone_output_dim, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(512, config.codebook_total_size),  # Logits for selection
        )

        # ==================== PROTOTYPE LEARNING ====================
        if config.enable_prototype_learning:
            # Learnable prototypes for each character class
            # Each character has a prototype in codebook space
            self.prototypes = nn.Parameter(
                torch.randn(num_classes, config.codebook_dim) / (config.codebook_dim**0.5)
            )

        # ==================== CLASSIFIER HEAD ====================
        # Instead of combining k codes, use backbone features directly for classification
        # This is more effective: backbone_output_dim=256 -> 512 -> 3036 classes
        self.classifier = nn.Sequential(
            nn.Linear(config.backbone_output_dim, 512),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(512),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes),
        )

    def gumbel_softmax_sample(
        self, logits: torch.Tensor, k: int, hard: bool = True
    ) -> torch.Tensor:
        """
        Gumbel-softmax trick for differentiable hard selection.

        ==================== WHY GUMBEL-SOFTMAX? ====================

        Problem: argmax is non-differentiable, can't backprop through selection
        Solution: Use Gumbel-softmax for differentiable approximation

        - During training: Soft selection (smooth gradients)
        - During inference: Hard selection (exactly k codes)
        - Temperature: Controls softness (low T -> hard, high T -> soft)
        """
        logits.size(0)

        # Add Gumbel noise
        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-20) + 1e-20)
        noisy_logits = logits + gumbel_noise

        # Softmax with temperature
        soft_selection = F.softmax(noisy_logits / self.config.temperature, dim=1)

        if hard:
            # Hard selection: Keep top-k, zero out rest
            topk_logits, topk_indices = torch.topk(soft_selection, k, dim=1)
            hard_selection = torch.zeros_like(soft_selection)
            hard_selection.scatter_(1, topk_indices, 1.0)

            # Straight-through estimator: use hard selection in forward,
            # but soft selection gradients in backward
            selection = hard_selection - soft_selection.detach() + soft_selection
        else:
            selection = soft_selection

        return selection

    def forward(self, x, num_active_codes: int = 5):
        """
        Forward pass with hierarchical codebook (simplified for performance).

        Parameters:
        - x: Input image (batch, 4096) or (batch, 1, 64, 64)
        - num_active_codes: k in multi-hot encoding (for future extensions)

        Returns:
        - logits: Character class predictions (batch, num_classes)
        """
        batch_size = x.size(0)

        # Reshape if needed
        if len(x.shape) == 2:
            x = x.view(batch_size, 1, 64, 64)

        # ==================== STAGE 1: FEATURE EXTRACTION ====================
        features = self.backbone(x)  # (batch, backbone_output_dim=256)

        # ==================== STAGE 2: CLASSIFICATION ====================
        # Use backbone features directly for classification
        # This provides sufficient capacity for 3036 classes
        logits = self.classifier(features)
        # (batch, num_classes)

        return logits


# ============================================================================
# TRAINING UTILITIES
# ============================================================================


class HierCodeTrainer:
    """Helper class for HierCode training"""

    def __init__(self, model: nn.Module, config: HierCodeConfig, device: str = "cuda"):
        self.model = model.to(device)
        self.config = config
        self.device = device
        self.history = {"train_loss": [], "val_loss": [], "train_acc": [], "val_acc": []}

    def train_epoch(self, train_loader: DataLoader, optimizer, criterion, epoch: int):
        """Train one epoch"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0

        with tqdm(train_loader, desc=f"Epoch {epoch} Train") as pbar:
            for images, labels in pbar:
                images = images.to(self.device)
                labels = labels.to(self.device)

                optimizer.zero_grad()

                # Forward pass with multi-hot encoding
                outputs = self.model(images, num_active_codes=self.config.multi_hot_k)
                loss = criterion(outputs, labels)

                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

                pbar.set_postfix(
                    {"loss": f"{loss.item():.4f}", "acc": f"{100 * correct / total:.1f}%"}
                )

        avg_loss = total_loss / len(train_loader)
        avg_acc = 100.0 * correct / total

        return avg_loss, avg_acc

    def validate(self, val_loader: DataLoader, criterion):
        """Validate model"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(self.device)
                labels = labels.to(self.device)

                outputs = self.model(images, num_active_codes=self.config.multi_hot_k)
                loss = criterion(outputs, labels)

                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

        avg_loss = total_loss / len(val_loader)
        avg_acc = 100.0 * correct / total

        return avg_loss, avg_acc

    def save_checkpoint(self, epoch: int, optimizer, scheduler, checkpoint_dir: Path):
        """
        Save training checkpoint with full state for resuming.

        Saves model state, optimizer state, scheduler state, epoch number,
        and training history for complete training recovery.
        """
        checkpoint_dir = Path(checkpoint_dir)
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{epoch:03d}.pt"

        checkpoint = {
            "epoch": epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "scheduler_state_dict": scheduler.state_dict(),
            "history": self.history,
        }

        torch.save(checkpoint, checkpoint_path)
        logger.info(f"  âœ“ Checkpoint saved: {checkpoint_path}")

        return checkpoint_path

    def load_checkpoint(self, checkpoint_path: Path, optimizer, scheduler):
        """
        Load training checkpoint to resume from a specific epoch.

        Restores model state, optimizer state, scheduler state, epoch number,
        and training history.
        """
        checkpoint_path = Path(checkpoint_path)
        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        self.model.load_state_dict(checkpoint["model_state_dict"])
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
        self.history = checkpoint["history"]

        epoch = checkpoint["epoch"]
        best_val_acc = max(self.history["val_acc"]) if self.history["val_acc"] else 0.0

        logger.info(f"âœ“ Checkpoint loaded: {checkpoint_path}")
        logger.info(f"  Resuming from epoch {epoch + 1}")
        logger.info(f"  Best validation accuracy so far: {best_val_acc:.2f}%")

        return epoch, best_val_acc


# ============================================================================
# MAIN TRAINING SCRIPT
# ============================================================================


def main():
    parser = argparse.ArgumentParser(
        description="HierCode (Hierarchical Codebook) Training for Kanji Recognition",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python train_hiercode.py
  python train_hiercode.py --codebook-total-size 1024 --hierarch-depth 10
  python train_hiercode.py --multi-hot-k 5 --backbone-type lightweight_cnn --epochs 50
  python train_hiercode.py --enable-zero-shot --zero-shot-radical-aware --resume-from training/hiercode/checkpoints/checkpoint_epoch_015.pt
        """,
    )

    # Dataset (auto-detected from common location)
    parser.add_argument("--sample-limit", type=int, default=None, help="Limit samples for testing")

    # Model
    parser.add_argument("--image-size", type=int, default=64, help="Input image size (default: 64)")
    parser.add_argument(
        "--num-classes",
        type=int,
        default=43528,
        help="Number of character classes (default: 43,528 for combined ETL6-9 dataset)",
    )

    # HierCode parameters
    parser.add_argument(
        "--codebook-total-size",
        type=int,
        default=1024,
        help="Total codebook size (default: 1024, must be <= 2^hierarch_depth)",
    )
    parser.add_argument(
        "--codebook-dim", type=int, default=128, help="Codebook dimension (default: 128)"
    )
    parser.add_argument(
        "--hierarch-depth",
        type=int,
        default=10,
        help="Hierarchical tree depth (default: 10 -> 1024 leaves)",
    )
    parser.add_argument(
        "--multi-hot-k",
        type=int,
        default=5,
        help="Number of active codewords (multi-hot k) (default: 5)",
    )
    parser.add_argument(
        "--temperature", type=float, default=0.1, help="Gumbel-softmax temperature (default: 0.1)"
    )

    # Backbone parameters
    parser.add_argument(
        "--backbone-type",
        type=str,
        default="lightweight_cnn",
        choices=["lightweight_cnn"],
        help="Backbone architecture (default: lightweight_cnn)",
    )
    parser.add_argument(
        "--backbone-output-dim",
        type=int,
        default=256,
        help="Backbone output dimension (default: 256)",
    )

    # Features
    parser.add_argument(
        "--enable-prototype-learning",
        action="store_true",
        help="Enable prototype learning for better classification",
    )
    parser.add_argument(
        "--enable-zero-shot",
        action="store_true",
        help="Enable zero-shot learning via radical decomposition",
    )
    parser.add_argument(
        "--zero-shot-radical-aware",
        action="store_true",
        help="Use radical-aware zero-shot learning",
    )

    # Training hyperparameters
    parser.add_argument(
        "--epochs", type=int, default=30, help="Total training epochs (default: 30)"
    )
    parser.add_argument("--batch-size", type=int, default=64, help="Batch size (default: 64)")
    parser.add_argument(
        "--learning-rate", type=float, default=0.001, help="Initial learning rate (default: 0.001)"
    )
    parser.add_argument(
        "--weight-decay", type=float, default=1e-4, help="L2 regularization (default: 1e-4)"
    )

    # Optimizer & scheduler
    parser.add_argument(
        "--optimizer",
        type=str,
        default="adamw",
        choices=["adamw", "sgd"],
        help="Optimizer (default: adamw)",
    )
    parser.add_argument(
        "--scheduler",
        type=str,
        default="cosine",
        choices=["cosine", "step"],
        help="LR scheduler (default: cosine)",
    )

    # Output
    parser.add_argument(
        "--model-dir",
        type=str,
        default="training/hiercode/config",
        help="Directory to save model config (default: training/hiercode/config)",
    )
    parser.add_argument(
        "--results-dir",
        type=str,
        default="training/hiercode/results",
        help="Directory to save results (default: training/hiercode/results)",
    )

    # Add checkpoint management arguments (unified across all scripts)
    setup_checkpoint_arguments(parser, "hiercode")

    args = parser.parse_args()

    # Auto-detect dataset directory
    data_dir = str(get_dataset_directory())
    logger.info(f"Using dataset from: {data_dir}")

    # ========== CREATE CONFIG ==========
    config = HierCodeConfig(
        data_dir=data_dir,
        image_size=args.image_size,
        num_classes=args.num_classes,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        codebook_total_size=args.codebook_total_size,
        codebook_dim=args.codebook_dim,
        hierarch_depth=args.hierarch_depth,
        multi_hot_k=args.multi_hot_k,
        temperature=args.temperature,
        backbone_type=args.backbone_type,
        backbone_output_dim=args.backbone_output_dim,
        enable_prototype_learning=args.enable_prototype_learning,
        enable_zero_shot=args.enable_zero_shot,
        zero_shot_radical_aware=args.zero_shot_radical_aware,
        optimizer=args.optimizer,
        scheduler=args.scheduler,
        model_dir=args.model_dir,
        results_dir=args.results_dir,
    )

    # ========== VERIFY GPU ==========
    verify_and_setup_gpu()

    logger.info("=" * 70)
    logger.info("HIERCODE (HIERARCHICAL CODEBOOK) TRAINING")
    logger.info("=" * 70)
    logger.info("ðŸ“‹ CONFIGURATION:")
    logger.info(f"  Data: {config.data_dir}")
    logger.info(f"  Epochs: {config.epochs}")
    logger.info(f"  Batch size: {config.batch_size}")
    logger.info(f"  Codebook: {config.codebook_total_size} codes (depth={config.hierarch_depth})")
    logger.info(f"  Multi-hot k: {config.multi_hot_k} active codewords")
    logger.info(f"  Backbone: {config.backbone_type} (output_dim={config.backbone_output_dim})")
    logger.info(f"  Prototype learning: {config.enable_prototype_learning}")
    logger.info(f"  Zero-shot learning: {config.enable_zero_shot}")
    logger.info(f"  Optimizer: {config.optimizer}, Scheduler: {config.scheduler}")

    # ========== LOAD DATA ==========
    logger.info("ðŸ“‚ LOADING DATASET...")
    X, y = load_chunked_dataset(config.data_dir)
    train_loader, val_loader, test_loader = create_data_loaders(
        X, y, config, sample_limit=args.sample_limit
    )

    # ========== CREATE MODEL ==========
    logger.info("ðŸ§  CREATING MODEL...")
    device = torch.device(config.device)
    model = HierCodeClassifier(num_classes=config.num_classes, config=config)

    # ========== TRAINING SETUP ==========
    criterion = nn.CrossEntropyLoss()
    optimizer = get_optimizer(model, config)
    scheduler = get_scheduler(optimizer, config)
    trainer = HierCodeTrainer(model, config, device=str(device))

    # ========== SAVE CONFIG ==========
    Path(config.model_dir).mkdir(parents=True, exist_ok=True)
    save_config(config, config.model_dir, "hiercode_config.json")

    # ========== INITIALIZE CHECKPOINT MANAGER ==========
    checkpoint_manager = CheckpointManager(args.checkpoint_dir, "hiercode")

    # ========== TRAINING LOOP ==========
    logger.info("ðŸš€ TRAINING...")
    best_val_acc = 0.0
    best_model_path = Path(config.model_dir) / "hiercode_model_best.pth"

    # Resume from checkpoint using unified DRY method
    start_epoch, best_metrics = checkpoint_manager.load_checkpoint_for_training(
        model,
        optimizer,
        scheduler,
        device,
        resume_from=args.resume_from,
        args_no_checkpoint=args.no_checkpoint,
    )
    best_val_acc = best_metrics.get("val_accuracy", 0.0)
    start_epoch = max(start_epoch, 1)  # Epoch numbering starts at 1

    for epoch in range(start_epoch, config.epochs + 1):
        train_loss, train_acc = trainer.train_epoch(train_loader, optimizer, criterion, epoch)
        val_loss, val_acc = trainer.validate(val_loader, criterion)

        trainer.history["train_loss"].append(train_loss)
        trainer.history["train_acc"].append(train_acc)
        trainer.history["val_loss"].append(val_loss)
        trainer.history["val_acc"].append(val_acc)

        scheduler.step()

        logger.info(
            f"Epoch {epoch}/{config.epochs} | "
            f"Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | "
            f"Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%"
        )

        if save_best_model(model, val_acc, best_val_acc, best_model_path):
            best_val_acc = val_acc

        # Save checkpoint after each epoch for resuming later
        checkpoint_manager.save_checkpoint(
            epoch, model, optimizer, scheduler, {"val_accuracy": val_acc}
        )

    # ========== TESTING ==========
    logger.info("ðŸ§ª TESTING...")
    load_best_model_for_testing(model, best_model_path, device)
    test_loss, test_acc = trainer.validate(test_loader, criterion)
    logger.info(f"Test Loss: {test_loss:.4f}, Acc: {test_acc:.2f}%")

    # ========== RESULTS ==========
    save_training_results(
        config,
        best_val_acc,
        test_acc,
        test_loss,
        trainer.history,
        Path(config.results_dir) / "hiercode_results.json",
    )

    # ========== CREATE CHARACTER MAPPING ==========
    logger.info("\nðŸ“Š Creating character mapping for inference...")
    try:
        from subprocess import run

        result = run(  # noqa: S603
            [
                sys.executable,
                "scripts/create_class_mapping.py",
                "--metadata-path",
                str(Path(config.data_dir) / "metadata.json"),
                "--output-dir",
                config.results_dir,
            ],
            capture_output=True,
            text=True,
        )
        if result.returncode == 0:
            logger.info("âœ“ Character mapping created successfully")
        else:
            logger.warning(f"âš  Character mapping creation failed: {result.stderr}")
    except Exception as e:
        logger.warning(f"âš  Could not create character mapping: {e}")

    logger.info("=" * 70)


if __name__ == "__main__":
    main()
